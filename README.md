## 1. Descargar el repositorio

 ```
git clone https://github.com/Cortijo02/GESTBD.git
 ```

## 2. Levantar contenedores

```
docker compose up postgres pgadmin elasticsearch graphdb jupyter
 ```

> Tambi√©n se puede levantar el contenedor jupyter_llm para revisar el procesamiento de los campos *descripcion* y *salidas* con llama3.1:8b.

En la carpeta `scripts` hemos dejado una serie de `.sh` que permiten lanzar los contenedores con diferentes configuraciones.

## 3. Acceder a los servicos

 En la siguiente tabla se detallan los puertos y accesos de los servicios:

| Servicio        | Puerto local | URL de acceso                      | Descripci√≥n                                |
|-----------------|---------------|------------------------------------|--------------------------------------------|
| üß≠ **PgAdmin**  | `8082`        | [http://localhost:8082](http://localhost:8082) | Interfaz web para gestionar Postgres      |
| üîç **Elasticsearch** | `8000`   | [http://localhost:8000](http://localhost:8000) | Motor de b√∫squeda y an√°lisis de texto     |
| üï∏Ô∏è **GraphDB**  | `8001`        | [http://localhost:8001](http://localhost:8001) | Base de datos de grafos RDF/SPARQL        |
| üìì **Jupyter**  | `8002`        | [http://localhost:8002](http://localhost:8002) | Entorno interactivo para notebooks Python |
| üìì **Jupyter LLMs**  | `8003`        | [http://localhost:8003](http://localhost:8003) | Entorno trabajar con jupyters, CUDA y LLMs (Ollama) |

Y a continuaci√≥n las credenciales de la base de datos:

| CAMPO               | VALOR                   |
|---------------------|-------------------------|
| email               | cinco@GESTDB.com        |
| password            | cinco                   |
| name                | postgres_db             |
| host name/address   | postgres                |
| port                | 5432                    |
| username_db         | userGESTDB              |
| password_db         | passGESTDB              |

En el contenedor de jupyter "queries.ipynb" contiene las queries a los servicios de Postgres, ElasticSearch y GraphDB.

## Sobre el trabajo

### Pasos seguidos

1. Web scraping: utilizamos los datos abiertos de la p√°gina de grados de la Comunidad de Madrid, que habilita PDFs con las descripciones, salidas y centros de los grados. Desde la p√°gina web base, hicimos web scraping pasa sacar las URLs de los PDFs, junto con las ramas y √°reas de las webs intermedias donde estaban ordenadas. De los PDFs sacamos la informaci√≥n relevante (jupyter > src > scraping > pdfs_automatizado.ipynb) y procesamos los datos (jupyter > src > scraping > generar_csvs.ipynb; jupyter > src > scraping > extraccion_centros.ipynb; jupyter > src > scraping > extraccion_notascorte.ipynb) para obtener los CSVs.

2. Postgres

# TODO

4. ElasticSearch

# TODO

6. GraphDB: Para realizar esto dise√±amos la ontolog√≠a estableciendo las tripletas (y los prefijos utilizados de ontolog√≠as existentes). A trav√©s de un script the python (jupyter > src > graph > generador_grafo.ipynb), pasamos de los datos de PostGres a la ontolog√≠a establecida y el resultado (se genera en esa misma carpeta con nombre universidadesMadrid.ttl)lo copiamos en (graphdb > imports). Una vez tenemos los archivos de la carpeta 'graphdb > imports' lanzamos el contenedor de GraphDB, generamos un repositorio 'Practica_GESTDB' e importamos ambos archivos (que se encuentrar en 'server files' porque los cargamos al montar el contenedor) con base iri 'http://example.org/universidadesMadrid#'. Una vez se realiza esto ya podemos lanzar las queries contra este sistema.

8. Cuaderno de consultas (jupyter > src > queries > queries.ipynb)

# TODO

### Futuros pasos y aspectos a mejorar

Una idea que superaba el scope de nuestro trabajo, pero que podr√≠a ser de inter√©s consistir√≠a en a√±adir una tabla con alumnos, datos personales y su carrera, e implementar un sistema de recomendaci√≥n en base a gustos u otros datos para ayudar a un determinado alumno a elegir un grado universitario. 

Uno de los retos de este proyecto ha sido la obtenci√≥n de datos desde los PDFs de la Comunidad de Madrid, que al tener m√∫ltiples formatos hac√≠a complicado en muchos casos aplicar reglas que favoreciesen la generalizaci√≥n y obtuviesen toda la informaci√≥n bien. Por ello, un aspecto a mejorar ser√≠a mejorar el procesamiento de los datos inciales y asegurar que estos tuviesen formatos similares en los CSVs resultantes.


### Aspectos legales

Los datos utilizados en este proyecto son utilizados con fines acad√©micos, han sido extra√≠dos de la p√°gina web de la Comunidad de Madrid el d√≠a 6 de noviembre de 2025.